{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score,precision_score, recall_score, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.impute import SimpleImputer\n",
    "import tensorflow.keras\n",
    "import keras.metrics\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History \n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"PTSD.xlsx\"\n",
    "df = pd.read_excel(path)\n",
    "df[\"drop\"] = df[\"PCL3\"].isna() & df[\"PCL2\"].isna() & ~df[\"PCL1\"].isna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  [ \"age\", \"highschool_diploma\", \"Hebrew\", \"dyslexia\", \"ADHD\", \"T1ETBE\", \"T1Acc1t\",\n",
    "                         \"T1Acc1n\", \"T1bias\", \"T2Acc1t\", \"T2Acc1n\", \"T2bias\", \"state1\", \"state2\", \"trait1\",\n",
    "                         \"trait2\", \"lot1\", \"lot2\", \"phq1\", \"phq2\", \"PCL1\", \"PCL2\", \"cd_risc1\", \"ptgi2\",\n",
    "                         \"active_coping1\", \"planning1\", \"positive_reframing1\", \"acceptance1\", \"humor1\",\n",
    "                         \"religion1\", \"emotional_support1\", \"instrumental_support1\", \"self_distraction1\",\n",
    "                         \"denial1\", \"venting1\", \"substance_use1\", \"behavioral_disengagement1\", \"self_blame1\",\n",
    "                         \"active_coping2\", \"planning2\", \"positive_reframing2\", \"acceptance2\", \"humor2\",\n",
    "                         \"religion2\", \"emotional_support2\", \"instrumental_support2\",\"self_distraction2\",\n",
    "                         \"denial2\", \"venting2\", \"substance_use2\", \"behavioral_disengagement2\", \"self_blame2\",\n",
    "                         \"trauma_history8_1\", \"military_exposure_unit\", \"HML_5HTT\", \"HL_MAOA\", \"HML_NPY\",\n",
    "                         \"COMT_Hap1_recode\", \"COMT_Hap2_recode\", \"COMT_Hap1_LvsMH\", \"HML_FKBP5\", \"Ashken_scale\",\n",
    "                         \"Sephar_scale\", \"Unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_features = [\"T1ETBE\", \"T1bias\", \"state1\", \"trait1\", \"phq1\", \"PCL1\",\n",
    "                \"denial1\", \"substance_use1\", \"self_blame1\",\n",
    "                         \"trauma_history8_1\", \"military_exposure_unit\", 't1bias_1_zero', \n",
    "                'state1_zero', 'trait1_zero', 'PHQ1_zero', 'PCL1_zero', 'ptsd1_clini', 'emotional_cop1n']\n",
    "numerical_features = [ \"T1ETBE\", \"T1Acc1t\", \"T1Acc1n\", \"T1bias\", \"T2Acc1t\",\"T2Acc1n\", \"T2bias\", \"state1\", \"state2\",\n",
    "                        \"trait1\", \"trait2\", \"lot1\", \"lot2\", \"phq1\", \"phq2\", \"cd_risc1\", \"PCL1\", \"PCL2\"]\n",
    "categorical_features = [ \"age\", \"highschool_diploma\", \"Hebrew\", \"dyslexia\", \"ADHD\",   \"ptgi2\",\n",
    "                    \"active_coping1\", \"planning1\", \"positive_reframing1\", \"acceptance1\", \"humor1\",\n",
    "                    \"religion1\", \"emotional_support1\", \"instrumental_support1\", \"self_distraction1\",\n",
    "                    \"denial1\", \"venting1\", \"substance_use1\", \"behavioral_disengagement1\", \"self_blame1\",\n",
    "                    \"active_coping2\", \"planning2\", \"positive_reframing2\", \"acceptance2\", \"humor2\",\n",
    "                    \"religion2\", \"emotional_support2\", \"instrumental_support2\", \"self_distraction2\",\n",
    "                    \"denial2\", \"venting2\", \"substance_use2\", \"behavioral_disengagement2\", \"self_blame2\",\n",
    "                    \"trauma_history8_1\", \"military_exposure_unit\", \"HML_5HTT\", \"HL_MAOA\", \"HML_NPY\",\n",
    "                    \"COMT_Hap1_recode\", \"COMT_Hap2_recode\", \"COMT_Hap1_LvsMH\", \"HML_FKBP5\", \"Ashken_scale\",\n",
    "                    \"Sephar_scale\", \"Unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for i, feature in enumerate(features):\n",
    "#     for interation in features[i::]:\n",
    "#         X[f\"interaction_{feature}_{interation}\"] = X[feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bad_features'] = (df > df.mean())[bad_features].sum(axis=1)\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "df[numerical_features] = imp.fit_transform(df[numerical_features])\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "df[categorical_features] = imp.fit_transform(df[categorical_features])\n",
    "\n",
    "X = df[features]\n",
    "\n",
    "Y = df[\"drop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X = ss.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.1, random_state=271828, stratify=Y)\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_train, y_train, test_size = 0.1, random_state=271828, stratify=y_train)\n",
    "#X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_train_2, y_train_2, test_size = 0.1, random_state=271828, stratify=y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(X_train, y_train):\n",
    "    X_train_3 = X_train[y_train==1]\n",
    "    y_train_3 = y_train[y_train==1]\n",
    "    X_train_4 = X_train[y_train==0][:10:]\n",
    "    y_train_4 = y_train[y_train==0][:10:]\n",
    "    X_train_5 = np.vstack((X_train_4, X_train_3))\n",
    "    y_train_5 =  np.hstack((y_train_4, y_train_3))\n",
    "    sm = SMOTE(random_state=27)\n",
    "    X_train_6, y_train_6 = sm.fit_sample(X_train_5, y_train_5.ravel())\n",
    "    X_train_6 = X_train_6[y_train_6==0]\n",
    "    y_train_6 = y_train_6[y_train_6==0]\n",
    "    return X_train_6, y_train_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "trues= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers 1 \n",
      "first_layer 20 \n",
      "each_layer 5 \n",
      "num_smote 1 \n",
      "loops 20\n",
      "\tscores f1 0.9333333333333333\n",
      "\tscores p 0.9333333333333333\n",
      "\tscores r 0.9333333333333333\n",
      "\tscores f1 train 0.9176470588235294\n",
      "\tscores p train 0.9285714285714286\n",
      "\tscores r train 0.9069767441860465\n",
      "num_layers 1 \n",
      "first_layer 20 \n",
      "each_layer 5 \n",
      "num_smote 1 \n",
      "loops 20\n",
      "\tscores f1 0.7499999999999999\n",
      "\tscores p 1.0\n",
      "\tscores r 0.6\n",
      "\tscores f1 train 0.9090909090909091\n",
      "\tscores p train 0.9493670886075949\n",
      "\tscores r train 0.872093023255814\n",
      "num_layers 1 \n",
      "first_layer 20 \n",
      "each_layer 5 \n",
      "num_smote 1 \n",
      "loops 20\n",
      "\tscores f1 0.5833333333333334\n",
      "\tscores p 0.7777777777777778\n",
      "\tscores r 0.4666666666666667\n",
      "\tscores f1 train 0.9221556886227545\n",
      "\tscores p train 0.9506172839506173\n",
      "\tscores r train 0.8953488372093024\n",
      "num_layers 1 \n",
      "first_layer 20 \n",
      "each_layer 5 \n",
      "num_smote 1 \n",
      "loops 20\n",
      "\tscores f1 0.8125000000000001\n",
      "\tscores p 0.7222222222222222\n",
      "\tscores r 0.9285714285714286\n",
      "\tscores f1 train 0.9371428571428572\n",
      "\tscores p train 0.9318181818181818\n",
      "\tscores r train 0.9425287356321839\n",
      "num_layers 1 \n",
      "first_layer 20 \n",
      "each_layer 5 \n",
      "num_smote 1 \n",
      "loops 20\n",
      "\tscores f1 0.8\n",
      "\tscores p 0.9090909090909091\n",
      "\tscores r 0.7142857142857143\n",
      "\tscores f1 train 0.9473684210526316\n",
      "\tscores p train 0.9642857142857143\n",
      "\tscores r train 0.9310344827586207\n",
      "num_layers 1 \n",
      "first_layer 20 \n",
      "each_layer 5 \n",
      "num_smote 1 \n",
      "loops 20\n",
      "\tscores f1 0.9655172413793104\n",
      "\tscores p 0.9333333333333333\n",
      "\tscores r 1.0\n",
      "\tscores f1 train 0.935672514619883\n",
      "\tscores p train 0.9523809523809523\n",
      "\tscores r train 0.9195402298850575\n",
      "num_layers 1 \n",
      "first_layer 20 \n",
      "each_layer 5 \n",
      "num_smote 1 \n",
      "loops 20\n",
      "\tscores f1 0.888888888888889\n",
      "\tscores p 0.9230769230769231\n",
      "\tscores r 0.8571428571428571\n",
      "\tscores f1 train 0.8809523809523809\n",
      "\tscores p train 0.9135802469135802\n",
      "\tscores r train 0.8505747126436781\n",
      "mean scores f1 0.8190818281335523\n",
      "mean scores p 0.8855477855477857\n",
      "mean scores r 0.7857142857142857\n",
      "mean scores f1 train 0.9214328329007065\n",
      "mean scores p train 0.9415172709325813\n",
      "mean scores r train 0.9025852522243862\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=7, shuffle=True)\n",
    "cvscores = []\n",
    "y_train_2 = np.array(y_train_2)\n",
    "X_train_2 = np.array(X_train_2)\n",
    "\n",
    "for num_layers in [1]:\n",
    "    for first_layer in [20]:\n",
    "        for loops in [20]:\n",
    "            for each_layer in [5]:\n",
    "                num_layers = num_layers\n",
    "                first_layer = first_layer\n",
    "                each_layer = each_layer\n",
    "                num_smote = 1\n",
    "                loops = loops\n",
    "                train_scores_f = []\n",
    "                train_scores_p = []\n",
    "                train_scores_r = []\n",
    "                        \n",
    "                scores_f = []\n",
    "                scores_p = []\n",
    "                scores_r = []\n",
    "                \n",
    "                for train, test in kfold.split(X_train_2, y_train_2):\n",
    "                    print(\"num_layers\", num_layers, \"\\nfirst_layer\", first_layer, \n",
    "                          \"\\neach_layer\", each_layer, \"\\nnum_smote\", num_smote, \"\\nloops\", loops)\n",
    "\n",
    "                    X_train_res = X_train_2[train]\n",
    "                    y_train_res = y_train_2[train]\n",
    "\n",
    "                  # create model\n",
    "                    n_cols = X_train_res.shape[1]\n",
    "                    model = Sequential()\n",
    "                    model.add(Dense(first_layer, activation='elu', input_dim = n_cols))\n",
    "                    model.add(Dropout(0.5))\n",
    "\n",
    "                    for i in range(num_layers):\n",
    "                        model.add(Dense(each_layer, activation='elu'))\n",
    "                        model.add(Dropout(0.5))\n",
    "\n",
    "                    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "                    model.compile(optimizer='adam', \n",
    "                                  loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "                    # Fit the model\n",
    "                    callbacks = [EarlyStopping(monitor='val_loss', patience=1)]\n",
    "                    model.fit(X_train_res, y_train_res, epochs = 150, validation_split = .1, verbose=0, callbacks=callbacks, class_weight= {1:0.45, 0:0.55})\n",
    "                    # evaluate the model\n",
    "                    y_pred =  model.predict(X_train_2[test])\n",
    "                    y_train_pred =  model.predict(X_train_2[train])\n",
    "                    preds.extend(y_pred)\n",
    "                    trues.extend(y_train_2[test])\n",
    "                    y_pred = y_pred>0.5\n",
    "                    s_f = f1_score(y_train_2[test], y_pred)\n",
    "                    s_p = precision_score(y_train_2[test], y_pred)\n",
    "                    s_r = recall_score(y_train_2[test], y_pred)\n",
    "                    print(\"\\tscores f1\", (s_f))\n",
    "                    print(\"\\tscores p\", (s_p))\n",
    "                    print(\"\\tscores r\", (s_r))\n",
    "                    scores_f.append(s_f)\n",
    "                    scores_p.append(s_p)\n",
    "                    scores_r.append(s_r)\n",
    "\n",
    "                    y_train_pred = (y_train_pred) > 0.5\n",
    "                    train_s_f = f1_score(y_train_2[train], y_train_pred)\n",
    "                    train_s_p = precision_score(y_train_2[train], y_train_pred)\n",
    "                    train_s_r = recall_score(y_train_2[train], y_train_pred)\n",
    "                    print(\"\\tscores f1 train\", (train_s_f))\n",
    "                    print(\"\\tscores p train\", (train_s_p))\n",
    "                    print(\"\\tscores r train\", (train_s_r))\n",
    "                    train_scores_f.append(train_s_f)\n",
    "                    train_scores_p.append(train_s_p)\n",
    "                    train_scores_r.append(train_s_r)\n",
    "                print(\"mean scores f1\", np.mean(scores_f))\n",
    "                print(\"mean scores p\", np.mean(scores_p))\n",
    "                print(\"mean scores r\", np.mean(scores_r))\n",
    "                        \n",
    "                print(\"mean scores f1 train\", np.mean(train_scores_f))\n",
    "                print(\"mean scores p train\", np.mean(train_scores_p))\n",
    "                print(\"mean scores r train\", np.mean(train_scores_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(preds)\n",
    "trues = np.array(trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=bool)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trues[np.where(preds > 0.8)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tscores f1 0.0\n",
      "\tscores p 0.0\n",
      "\tscores r 0.0\n",
      "\tscores f1 train 0.0\n",
      "\tscores p train 0.0\n",
      "\tscores r train 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "num_layers = 2\n",
    "first_layer = 75\n",
    "each_layer = 10\n",
    "\n",
    "# create model\n",
    "n_cols = X_train_res.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(first_layer, activation='elu', input_dim = n_cols))\n",
    "model.add(Dropout(0.5))\n",
    "for i in range(num_layers):\n",
    "    model.add(Dense(each_layer, activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "                                  loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "                    # Fit the model\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "model.fit(X_train_2, y_train_2, epochs = 150, validation_split = .1, verbose=0, callbacks=callbacks, class_weight = {1:3, 0:7})\n",
    "                    # evaluate the model\n",
    "y_pred =  model.predict(X_test_2)\n",
    "y_train_pred =  model.predict(X_train_2)\n",
    "#preds.extend(y_pred)\n",
    "#trues.extend(y_train_2[test])\n",
    "y_pred = y_pred>0.5\n",
    "s_f = f1_score(y_test_2, y_pred)\n",
    "s_p = precision_score(y_test_2, y_pred)\n",
    "s_r = recall_score(y_test_2, y_pred)\n",
    "print(\"\\tscores f1\", (s_f))\n",
    "print(\"\\tscores p\", (s_p))\n",
    "print(\"\\tscores r\", (s_r))\n",
    "                    \n",
    "y_train_pred = (y_train_pred) > 0.5\n",
    "train_s_f = f1_score(y_train_2, y_train_pred)\n",
    "train_s_p = precision_score(y_train_2, y_train_pred)\n",
    "train_s_r = recall_score(y_train_2, y_train_pred)\n",
    "print(\"\\tscores f1 train\", (train_s_f))\n",
    "print(\"\\tscores p train\", (train_s_p))\n",
    "print(\"\\tscores r train\", (train_s_r))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
