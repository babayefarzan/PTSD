{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_iris()\n",
    "y = (df.target == 0) #- Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n\n",
    "X = df.data #- sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "        self.B = np.random.rand()\n",
    "        self.learning_rate = 0.000001\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.W = np.random.rand(X.shape[1])\n",
    "        for i in range (200000):\n",
    "            a = sigmoid(np.dot(self.W, X.T) + self.B)            \n",
    "            b_gradient = (1 / X.shape[0]) * np.sum(a - y)\n",
    "            w_gradient = (1 / X.shape[0]) *np.dot((a - y), X)\n",
    "            \n",
    "            self.B -= self.learning_rate * b_gradient\n",
    "            self.W -= self.learning_rate * w_gradient\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return sigmoid(np.dot(self.W, X.T) + self.B)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, False, True, False, True, False, False, False, False, True, False, False, False, True, False, True, True, False, False, True, True, False, False, False, False, True, False, False, True, True, False, False, True, True, False, True]\n",
      "[ True  True  True False  True False  True False False False False  True\n",
      " False False False  True False  True  True False False  True  True False\n",
      " False False False  True False False  True  True False False  True  True\n",
      " False  True]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = []\n",
    "for x in X_test:\n",
    "    y_pred.append(lr.predict(x) > 0.5)\n",
    "#a_s = accuracy_score(y_test, y_pred)\n",
    "print(y_pred)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X11 = x\n",
    "X12 = y\n",
    "\n",
    "3 hidden - X21, X22, X23\n",
    "\n",
    "1 output - X31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self):\n",
    "        self.x21 = LogisticRegression()\n",
    "        self.x22 = LogisticRegression()\n",
    "        self.x31 = LogisticRegression()\n",
    "        \n",
    "    def forward_pass(self, X):\n",
    "        a2 = np.array([self.x21.predict(X), self.x22.predict(X)])\n",
    "        a3 = np.array(self.x31.predict(a2.T))\n",
    "        return a2, a3\n",
    "    \n",
    "    def Backpropagation(self, X):\n",
    "        delta2_3 = (a3 - y) * a3 * (1-a3)\n",
    "        w2_3 = np.dot(delta2_3, a2.transpose())\n",
    "        \n",
    "        \n",
    "        delta1_2 = np.dot(self.x31.W.transpose, delta2_3) * a2 * (1-a2)\n",
    "        w1_2 = np.dot(delta1_2, X.transpose())\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.x21.W = np.random.rand(X.shape[1])\n",
    "        self.x22.W = np.random.rand(X.shape[1])\n",
    "        self.x31.W = np.random.rand(2)\n",
    "        \n",
    "        for i in range (2000):\n",
    "            \n",
    "            a2, a3 = self.forward_pass(X)\n",
    "            \n",
    "            delta2_3 = (a3 - y) * a3 * (1-a3)\n",
    "            w2_3 = np.dot(delta2_3, a2.transpose())\n",
    "\n",
    "            print (\"self.x31.W.transpose\", self.x31.W.transpose())\n",
    "            print(\"delta2_3\", delta2_3)\n",
    "            delta1_2 = np.dot(self.x31.W.transpose(), delta2_3) #* a2 * (1-a2)\n",
    "            w1_2 = np.dot(delta1_2, X.transpose())\n",
    "\n",
    "            self.x31.B -= self.learning_rate * delta2_3\n",
    "            self.x31.W -= learning_rate * w2_3\n",
    "            \n",
    "            self.x22.B -= self.learning_rate * delta1_2[1]\n",
    "            self.x22.W -= learning_rate * w1_2[1]\n",
    "            \n",
    "            self.x21.B -= self.learning_rate * delta1_2[0]\n",
    "            self.x21.W -= learning_rate * w1_2[0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        a2 = np.array([self.x21.predict(X), self.x22.predict(X)])\n",
    "        a3 = self.x31.predict(a2)\n",
    "        return a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.x31.W.transpose [0.69044878 0.5058243 ]\n",
      "delta2_3 [ 0.10467395  0.10467692  0.10467878 -0.01748154  0.10467089  0.10467485\n",
      " -0.01747743 -0.01747556  0.10468001  0.10467219 -0.01743674  0.10467118\n",
      "  0.10467188  0.10467504 -0.01743789 -0.01746453 -0.01742276  0.10467541\n",
      " -0.01742203 -0.01740356 -0.01744556  0.10467067 -0.01748432  0.10467611\n",
      " -0.01747556  0.10467297  0.10470056  0.10470301  0.10467973  0.10468635\n",
      "  0.10467204  0.10468715 -0.01740881  0.10468582  0.10469601  0.10468442\n",
      "  0.1046749  -0.0174219  -0.01744832 -0.01741705  0.10467231  0.10469044\n",
      " -0.01749549  0.10467433  0.10467614  0.10467522 -0.01746073  0.10467294\n",
      "  0.10467204  0.10468401  0.10467467  0.10468468 -0.01741515  0.10467143\n",
      "  0.10467123  0.10467114  0.10467955  0.10469159  0.10467533 -0.01740483\n",
      " -0.01742538  0.10468488  0.10467616 -0.01748507  0.10468045  0.10467622\n",
      "  0.10472811 -0.01756097 -0.01745786  0.1046708   0.10467811  0.10467218\n",
      " -0.01748068  0.10467529 -0.01743007  0.10467058  0.10469169  0.10467806\n",
      " -0.01743599  0.10467159  0.10467543  0.10467046 -0.01751623  0.10468055\n",
      "  0.10467148 -0.01744628  0.10468082 -0.01750063  0.10467507 -0.01742293\n",
      "  0.10467176 -0.0174441   0.10468215  0.10467206 -0.01740275  0.10469433\n",
      " -0.01747623  0.10469141 -0.0174238   0.10473164 -0.01742067 -0.01751738\n",
      "  0.10467761  0.10468225  0.10467309 -0.01741469  0.10467371 -0.01746218\n",
      "  0.10470526 -0.01744269  0.10467069 -0.01746285]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,) and (112,) not aligned: 2 (dim 0) != 112 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a8df8779a722>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-35ed246e6eea>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"self.x31.W.transpose\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx31\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"delta2_3\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta2_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mdelta1_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx31\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta2_3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#* a2 * (1-a2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0mw1_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta1_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (2,) and (112,) not aligned: 2 (dim 0) != 112 (dim 0)"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "nn = NN()\n",
    "nn.fit(X_train, y_train)\n",
    "y_pred = []\n",
    "for x in X_test:\n",
    "    y_pred.append(nn.predict(x))\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "...\n",
    "   def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple \"(nabla_b, nabla_w)\" representing the\n",
    "        gradient for the cost function C_x.  \"nabla_b\" and\n",
    "        \"nabla_w\" are layer-by-layer lists of numpy arrays, similar\n",
    "        to \"self.biases\" and \"self.weights\".\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "...\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y) \n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
